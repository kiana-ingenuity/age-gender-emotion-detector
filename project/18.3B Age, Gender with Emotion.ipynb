{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age, Gender and Emotion Detection\n",
    "\n",
    "### Let's load our classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import dlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "from contextlib import contextmanager\n",
    "from wide_resnet import WideResNet\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "\n",
    "#We use pretrained model VGG,(add keras Pre-trained Models) \n",
    "#We first load the model architecture and pre-trained weights\n",
    "#We then add a task-specific classification layer on the top of the pre-trained model.\n",
    "# There is a file containing the emotion values for all the data.\n",
    "\n",
    "classifier = load_model('./model/emotion_little_vgg_2.h5')\n",
    "pretrained_model = './Weights/weights.28-3.73.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Age, Gender, Emotion of people in any Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Define Image Path Here\n",
    "image_path = \"./images/\"\n",
    "\n",
    "emotion_classes = {0: 'Angry ' , 1: 'Fear', 2: 'Happy' , 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n",
    "\n",
    "\n",
    "#draw rectangle and put information text top,left of images\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_PLAIN,\n",
    "               font_scale=1, thickness=1):\n",
    "    \n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    \n",
    "    #It is the coordinates of the text string in the image.\n",
    "    x, y = point\n",
    "    \n",
    "    #the rectangle above img and its position and colour.\n",
    "    #It is the color of text string to be drawn. For BGR, we pass a tuple\n",
    "    cv2.rectangle(image, (x+size[0], y), (x, y-size[1]), (0, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (100, 50, 255), thickness, lineType=cv2.LINE_AA)#the information text\n",
    "    \n",
    "\n",
    "# Define our model parameters\n",
    "depth = 16\n",
    "k = 8       #K:the widening factor, which multiplies the number of feature channels in each convolution layer.\n",
    "weight_file = None\n",
    "margin = 0.4    #for imdb\n",
    "image_dir = None\n",
    "\n",
    "\n",
    "# Get our weight file \n",
    "if not weight_file:\n",
    "    weight_file = get_file(\"weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
    "                              cache_dir=Path(sys.argv[0]).resolve().parent)\n",
    "\n",
    "# load model and weights\n",
    "\n",
    "img_size = 64  #default 64\n",
    "\n",
    "model = WideResNet(img_size, depth=depth, k=k)()\n",
    "model.load_weights(weight_file)\n",
    "\n",
    "\n",
    "#The get_frontal_face_detector() will return a detector that is a function we can use to retrieve the faces information.\n",
    "#Each face is an object that contains the points where the image can be found.\n",
    "detector = dlib.get_frontal_face_detector() # create the detector\n",
    "\n",
    "#create a list (f for f in)\n",
    "#method listdir() returns a list containing the names of the entries in the directory given by path\n",
    "image_names = [f for f in listdir(image_path) if isfile(join(image_path, f))]\n",
    "\n",
    "\n",
    "for image_name in image_names:\n",
    "    frame = cv2.imread(\"./images/\" + image_name) # read the image\n",
    "    \n",
    "    preprocessed_faces_emo = []           \n",
    " \n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert image into grayscale(opencv face detector expects gray images)\n",
    "    \n",
    "    \n",
    "    #numpy.shape() function finds the shape of an array. By shape, we mean that it helps in finding the dimensions of an array.\n",
    "    img_h, img_w, _ = np.shape(input_img)  #Return the shape of an array.\n",
    "    \n",
    "    \n",
    "    # Use detector (dlib) to find face\n",
    "    detected = detector(frame, 1) \n",
    "    \n",
    "     # placeholder for cropped faces\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    #len:Return the length (the number of items) of an object.\n",
    "    \n",
    "    preprocessed_faces_emo = []\n",
    "    if len(detected) > 0:\n",
    "        for i, d in enumerate(detected):   #Return an enumerate object. iterable must be a sequence.\n",
    "           \n",
    "            x1 = d.left()       # left point\n",
    "            y1 = d.top()        # top point\n",
    "            x2 = d.right() + 1  # right point\n",
    "            y2 = d.bottom() + 1 # bottom point\n",
    "            w = d.width()       # width point\n",
    "            h = d.height()      # height point\n",
    "           \n",
    "        #set all of the properties for the four margins \n",
    "        #add some margin to the face detected area to include a full head\n",
    "            xw1 = max(int(x1 - margin * w), 0)\n",
    "            yw1 = max(int(y1 - margin * h), 0)\n",
    "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "           \n",
    "            # Draw a rectangle(position, colour, thickness)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 2)\n",
    "            faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "            face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
    "            \n",
    "            face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY) # change to greyscale\n",
    "            face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "           \n",
    "           # Convert the field to a float\n",
    "           # Regarding the division by 255, this is the maximum value of a byte \n",
    "           #(the input feature's type before the conversion to float32), so this will ensure \n",
    "           #that the input features are scaled between 0.0 and 1.0. \n",
    "            face_gray_emo = face_gray_emo.astype(\"float\") / 255.0    \n",
    "            \n",
    "            #Keras provides the img_to_array() function for converting a loaded image in PIL format\n",
    "            #into a NumPy array for use with deep learning models\n",
    "            face_gray_emo = img_to_array(face_gray_emo)\n",
    "            \n",
    "            #Expand the shape of an array.\n",
    "            #Insert a new axis that will appear at the axis position in the expanded array shape.\n",
    "            face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\n",
    "            preprocessed_faces_emo.append(face_gray_emo)\n",
    "\n",
    "       \n",
    "    \n",
    "       # make a prediction for Age and Gender\n",
    "        results = model.predict(np.array(faces)) # make a prediction\n",
    "        predicted_genders = results[0]\n",
    "        ages = np.arange(0, 101).reshape(101, 1) # shape arr with 101 rows and 1 columns (It shapes an array without changing the data of array.)\n",
    "        predicted_ages = results[1].dot(ages).flatten() #flatten:Return a copy of the array collapsed into one dimension\n",
    "\n",
    "      \n",
    "        # make a prediction for Emotion \n",
    "        emo_labels = []\n",
    "        for i, d in enumerate(detected):\n",
    "            preds = classifier.predict(preprocessed_faces_emo[i])[0]\n",
    "            emo_labels.append(emotion_classes[preds.argmax()])\n",
    "            \n",
    "      \n",
    "    \n",
    "      # draw results\n",
    "        for i, d in enumerate(detected):\n",
    "            label = \"{}, {}, {}\".format(int(predicted_ages[i]),\n",
    "                                \"Female\" if predicted_genders[i][0] > 0.5 else \"Male\", emo_labels[i] )\n",
    "           \n",
    "        draw_label(frame, (d.left(), d.top()), label)\n",
    "\n",
    "   \n",
    "\n",
    "    cv2.imshow(\"A,G,E Detector\", frame) # show the image\n",
    "    filename = \"output_images/\"+image_name\n",
    "    \n",
    "    cv2.imwrite(filename,frame) # save result\n",
    "    \n",
    "    \n",
    "    #we need to pause execution, as the window will be destroyed when the script stops, \n",
    "    #so we use cv2.waitKey to hold the window until a key is pressed, and after that,\n",
    "    #we destroy the window and exit the script.\n",
    "    \n",
    "    cv2.waitKey(0)  # Wait for a key press to exit\n",
    "    cv2.destroyAllWindows()   # Close all windows   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection with webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Define Image Path Here\n",
    "#image_path = \"./images/\"\n",
    "\n",
    "emotion_classes = {0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n",
    "\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=0.8, thickness=1):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (0, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (100, 50, 255), thickness, lineType=cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "# Define our model parameters\n",
    "depth = 16\n",
    "k = 8\n",
    "weight_file = None\n",
    "margin = 0.4\n",
    "image_dir = None\n",
    "\n",
    "# Get our weight file \n",
    "if not weight_file:\n",
    "    weight_file = get_file(\"weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
    "                                cache_dir=Path(sys.argv[0]).resolve().parent)\n",
    "# load model and weights\n",
    "img_size = 64\n",
    "model = WideResNet(img_size, depth=depth, k=k)()\n",
    "model.load_weights(weight_file)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Webcam (0 means the default video capture device in OS)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Capture frame-by-frame\n",
    "  \n",
    "\n",
    "    preprocessed_faces_emo = []           \n",
    " \n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w, _ = np.shape(input_img)\n",
    "    detected = detector(frame, 1)\n",
    "    \n",
    "     # placeholder for cropped faces\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    \n",
    "    preprocessed_faces_emo = []\n",
    "    if len(detected) > 0:\n",
    "        for i, d in enumerate(detected):\n",
    "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "            xw1 = max(int(x1 - margin * w), 0)\n",
    "            yw1 = max(int(y1 - margin * h), 0)\n",
    "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            \n",
    "            faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "            face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
    "            face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "            face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\n",
    "            face_gray_emo = img_to_array(face_gray_emo)\n",
    "            face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\n",
    "            preprocessed_faces_emo.append(face_gray_emo)\n",
    "\n",
    "        # make a prediction for Age and Gender\n",
    "        results = model.predict(np.array(faces))\n",
    "        predicted_genders = results[0]\n",
    "        ages = np.arange(0, 101).reshape(101, 1)\n",
    "        predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "        # make a prediction for Emotion \n",
    "        emo_labels = []\n",
    "        for i, d in enumerate(detected):\n",
    "            preds = classifier.predict(preprocessed_faces_emo[i])[0]\n",
    "            emo_labels.append(emotion_classes[preds.argmax()])\n",
    "        \n",
    "        # draw results\n",
    "        for i, d in enumerate(detected):\n",
    "            label = \"{}, {}, {}\".format(int(predicted_ages[i]),\n",
    "                                        \"Female\" if predicted_genders[i][0] > 0.5 else \"Male\", emo_labels[i])\n",
    "            draw_label(frame, (d.left(), d.top()), label)\n",
    "\n",
    "    cv2.imshow(\"A,G,E Detector\", frame)\n",
    "    if cv2.waitKey(5) == 27:  # ESC key press\n",
    "        break\n",
    "\n",
    "cap.release()  # When everything is done, release the capture\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
